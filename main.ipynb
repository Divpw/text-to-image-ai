{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Stable Diffusion Image Generation with Gradio UI\n",
    "\n",
    "This notebook allows you to generate images from text prompts using Stable Diffusion with an enhanced user interface. It runs on a free Google Colab GPU and provides a Gradio interface for interaction.\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Enable GPU:** Go to `Runtime` -> `Change runtime type` and select `GPU` (e.g., T4) as the hardware accelerator.\n",
    "2.  **Run Cells:** Execute each cell in order.\n",
    "    *   **Cell 1:** Installs necessary libraries.\n",
    "    *   **Cell 2:** Imports libraries, defines style presets, helper functions, and the core image generation logic.\n",
    "    *   **Cell 3:** Defines the Gradio User Interface structure.\n",
    "    *   **Cell 4:** Loads the Stable Diffusion model (this may take a few minutes, especially on the first run as it downloads model weights). Choose the model to load here.\n",
    "    *   **Cell 5:** Launches the Gradio interface. Click the public URL it provides to open the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title 1. Install Dependencies\n",
    "# We need diffusers, transformers, accelerate for Stable Diffusion,\n",
    "# gradio for the UI, and bitsandbytes for potential 8-bit optimization (optional but good to have).\n",
    "!pip install diffusers transformers accelerate gradio bitsandbytes --quiet\n",
    "print(\"Dependencies installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries, Define Presets, Helpers, and Generation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title 2. Imports, Presets, Helpers, and Generation Logic\n",
    "import gradio as gr\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler, AutoPipelineForText2Image\n",
    "from PIL import Image\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import sys # For checking if in Colab\n",
    "import traceback # For detailed error logging\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# --- Global Variables ---\n",
    "pipe = None\n",
    "current_model_id = None # Will be set in cell 4\n",
    "DEFAULT_COLAB_MODEL_ID = \"runwayml/stable-diffusion-v1-5\" # Good default for free Colab tier\n",
    "# For SDXL (might be too heavy for free Colab T4, consider A100 if available):\n",
    "# DEFAULT_COLAB_MODEL_ID = \"stabilityai/sdxl-base-1.0\"\n",
    "\n",
    "# --- Style Presets ---\n",
    "STYLE_PRESETS = {\n",
    "    \"None\": {\"prompt_suffix\": \"\", \"negative_prompt_prefix\": \"\"},\n",
    "    \"Realistic\": {\"prompt_suffix\": \"photorealistic, 4k, ultra detailed, cinematic lighting, professional photography\", \"negative_prompt_prefix\": \"cartoon, anime, drawing, sketch, stylized, illustration, painting, art\"},\n",
    "    \"Anime\": {\"prompt_suffix\": \"anime style, key visual, vibrant, beautiful, detailed, official art, by makoto shinkai\", \"negative_prompt_prefix\": \"photorealistic, 3d render, ugly, disfigured, real life\"},\n",
    "    \"Fantasy Art\": {\"prompt_suffix\": \"fantasy art, detailed, intricate, epic, trending on artstation, by greg rutkowski, Brom, Frank Frazetta\", \"negative_prompt_prefix\": \"photorealistic, modern, simple, photo\"},\n",
    "    \"Digital Painting\": {\"prompt_suffix\": \"digital painting, concept art, smooth, sharp focus, illustration, detailed\", \"negative_prompt_prefix\": \"photo, 3d model, realism, ugly\"},\n",
    "    \"3D Render\": {\"prompt_suffix\": \"3d render, octane render, blender, detailed, physically based rendering, vray\", \"negative_prompt_prefix\": \"2d, drawing, sketch, painting, illustration, flat\"},\n",
    "}\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def get_random_seed():\n",
    "    return random.randint(0, 2**32 - 1)\n",
    "\n",
    "def apply_style(prompt, style_name):\n",
    "    if style_name == \"None\" or style_name not in STYLE_PRESETS:\n",
    "        return prompt, \"\"\n",
    "    \n",
    "    preset = STYLE_PRESETS[style_name]\n",
    "    styled_prompt = f\"{prompt.strip()}, {preset['prompt_suffix']}\" if prompt.strip() else preset['prompt_suffix']\n",
    "    return styled_prompt.strip(\", \"), preset['negative_prompt_prefix'].strip(\", \")\n",
    "\n",
    "# --- Google Drive Mounting (if in Colab) ---\n",
    "def mount_google_drive():\n",
    "    if 'google.colab' in sys.modules:\n",
    "        try:\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive')\n",
    "            print(\"Google Drive mounted successfully at /content/drive\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error mounting Google Drive: {e}\")\n",
    "            return False\n",
    "    return False # Not in Colab or failed\n",
    "GDRIVE_MOUNTED_SUCCESSFULLY = False # Global flag\n",
    "GDRIVE_SAVE_PATH = \"/content/drive/MyDrive/AI_Generated_Images/StableDiffusion/\"\n",
    "\n",
    "# --- Model Loading Function (Colab specific status updates) ---\n",
    "def load_model_colab(model_id_to_load, use_float16=True, use_attention_slicing=False, status_gr_textbox_ref=None):\n",
    "    \"\"\"Loads the model. status_gr_textbox_ref is a list containing the Gradio Textbox instance.\"\"\"\n",
    "    global pipe, current_model_id\n",
    "\n",
    "    def update_status(message):\n",
    "        print(message) # Always print to console\n",
    "        # UI update will be handled by yield\n",
    "\n",
    "    if pipe is not None and current_model_id == model_id_to_load:\n",
    "        update_status(f\"Model '{model_id_to_load}' already loaded.\")\n",
    "        yield gr.update(value=f\"Model '{model_id_to_load}' already loaded.\")\n",
    "        return\n",
    "\n",
    "    update_status(f\"Loading model: {model_id_to_load}...\")\n",
    "    yield gr.update(value=f\"Loading model: {model_id_to_load}...\")\n",
    "\n",
    "    pipeline_args = {}\n",
    "    if torch.cuda.is_available() and use_float16:\n",
    "        pipeline_args[\"torch_dtype\"] = torch.float16\n",
    "        update_status(\"Using float16 precision.\")\n",
    "    \n",
    "    model_is_sdxl = \"sdxl\" in model_id_to_load.lower()\n",
    "\n",
    "    try:\n",
    "        if model_is_sdxl:\n",
    "            update_status(f\"Loading SDXL model: {model_id_to_load}...\")\n",
    "            yield gr.update(value=f\"Loading SDXL model: {model_id_to_load}...\")\n",
    "            pipe = AutoPipelineForText2Image.from_pretrained(model_id_to_load, **pipeline_args)\n",
    "        else:\n",
    "            update_status(f\"Loading Stable Diffusion model: {model_id_to_load}...\")\n",
    "            yield gr.update(value=f\"Loading Stable Diffusion model: {model_id_to_load}...\")\n",
    "            scheduler = EulerDiscreteScheduler.from_pretrained(model_id_to_load, subfolder=\"scheduler\")\n",
    "            pipeline_args[\"scheduler\"] = scheduler\n",
    "            pipe = StableDiffusionPipeline.from_pretrained(model_id_to_load, **pipeline_args)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            update_status(\"Moving model to CUDA.\")\n",
    "            pipe = pipe.to(\"cuda\")\n",
    "        else:\n",
    "            update_status(\"CUDA not available. Running on CPU (this will be very slow). Warning: May not work.\")\n",
    "\n",
    "        if use_attention_slicing and hasattr(pipe, \"enable_attention_slicing\"):\n",
    "            update_status(\"Enabling attention slicing.\")\n",
    "            pipe.enable_attention_slicing()\n",
    "        \n",
    "        current_model_id = model_id_to_load\n",
    "        update_status(f\"Model '{model_id_to_load}' loaded successfully.\")\n",
    "        yield gr.update(value=f\"Model '{model_id_to_load}' loaded successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error loading model '{model_id_to_load}': {str(e)}\"\n",
    "        update_status(error_message)\n",
    "        traceback.print_exc()\n",
    "        pipe = None\n",
    "        current_model_id = None\n",
    "        yield gr.update(value=f\"{error_message}. Check console.\")\n",
    "\n",
    "# --- Image Generation Function ---\n",
    "def generate_image_colab_fn(prompt, negative_prompt, style_name, num_inference_steps, guidance_scale, seed_value, \n",
    "                            custom_filename_prefix_val, save_to_gdrive_val, \n",
    "                            progress=gr.Progress(track_ œÑœåœÑŒµ=True)):\n",
    "    global pipe, current_model_id, GDRIVE_MOUNTED_SUCCESSFULLY, GDRIVE_SAVE_PATH\n",
    "    \n",
    "    additional_info = []\n",
    "\n",
    "    if pipe is None:\n",
    "        return None, \"Model not loaded. Please run the model loading cell first.\", gr.DownloadButton.update(visible=False), \"\"\n",
    "\n",
    "    progress(0, desc=\"üé® Starting generation...\")\n",
    "\n",
    "    styled_prompt, style_negative_prefix = apply_style(prompt, style_name)\n",
    "    if style_negative_prefix and negative_prompt:\n",
    "        final_negative_prompt = f\"{style_negative_prefix}, {negative_prompt}\"\n",
    "    elif style_negative_prefix:\n",
    "        final_negative_prompt = style_negative_prefix\n",
    "    else:\n",
    "        final_negative_prompt = negative_prompt\n",
    "\n",
    "    progress(0.1, desc=f\"üìù Prompt: {styled_prompt[:100]}...\")\n",
    "    print(f\"Generating with Prompt: '{styled_prompt}'\")\n",
    "    if final_negative_prompt: print(f\"Negative Prompt: '{final_negative_prompt}'\")\n",
    "    \n",
    "    try:\n",
    "        seed = int(seed_value)\n",
    "    except (ValueError, TypeError):\n",
    "        seed = get_random_seed()\n",
    "    print(f\"üå± Seed: {seed}\")\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    generator = torch.Generator(device).manual_seed(seed)\n",
    "    \n",
    "    num_inference_steps = int(num_inference_steps)\n",
    "    guidance_scale = float(guidance_scale)\n",
    "\n",
    "    generation_args = {\n",
    "        \"prompt\": styled_prompt,\n",
    "        \"negative_prompt\": final_negative_prompt if final_negative_prompt else None,\n",
    "        \"num_inference_steps\": num_inference_steps,\n",
    "        \"guidance_scale\": guidance_scale,\n",
    "        \"generator\": generator\n",
    "    }\n",
    "\n",
    "    image = None\n",
    "    try:\n",
    "        for i in progress.tqdm(range(num_inference_steps), desc=\" Diffusion steps\"):\n",
    "            if i == num_inference_steps - 1:\n",
    "                if device == \"cuda\" and hasattr(pipe, 'torch_dtype') and pipe.torch_dtype == torch.float16:\n",
    "                    with torch.autocast(\"cuda\"):\n",
    "                        image = pipe(**generation_args).images[0]\n",
    "                else:\n",
    "                    image = pipe(**generation_args).images[0]\n",
    "        \n",
    "        if image is None: raise RuntimeError(\"Image generation failed within loop.\")\n",
    "\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        sane_prompt_prefix = \"\".join(c if c.isalnum() else \"_\" for c in styled_prompt[:30])\n",
    "        filename_prefix_to_use = custom_filename_prefix_val.strip() if custom_filename_prefix_val.strip() else sane_prompt_prefix\n",
    "        base_filename = f\"{filename_prefix_to_use}_{current_model_id.split('/')[-1]}_{seed}_{timestamp}.png\"\n",
    "\n",
    "        print(f\"‚úÖ Image generation successful. Filename: {base_filename}\")\n",
    "        progress(1.0, desc=\"üéâ Generation Complete!\")\n",
    "        \n",
    "        # Prepare image for download button (local temp save)\n",
    "        temp_dir = \"temp_generated_colab_images\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        local_img_path = os.path.join(temp_dir, base_filename)\n",
    "        image.save(local_img_path)\n",
    "        additional_info.append(f\"Saved locally for download: {local_img_path}\")\n",
    "\n",
    "        # Save to Google Drive if requested and possible\n",
    "        if save_to_gdrive_val and 'google.colab' in sys.modules:\n",
    "            if not GDRIVE_MOUNTED_SUCCESSFULLY:\n",
    "                print(\"Attempting to mount Google Drive for saving...\")\n",
    "                GDRIVE_MOUNTED_SUCCESSFULLY = mount_google_drive()\n",
    "            \n",
    "            if GDRIVE_MOUNTED_SUCCESSFULLY:\n",
    "                try:\n",
    "                    os.makedirs(GDRIVE_SAVE_PATH, exist_ok=True)\n",
    "                    gdrive_img_path = os.path.join(GDRIVE_SAVE_PATH, base_filename)\n",
    "                    image.save(gdrive_img_path)\n",
    "                    print(f\"üíæ Image saved to Google Drive: {gdrive_img_path}\")\n",
    "                    additional_info.append(f\"Saved to GDrive: {gdrive_img_path}\")\n",
    "                except Exception as e:\n",
    "                    gdrive_save_error = f\"‚ùå Error saving to Google Drive: {e}\"\n",
    "                    print(gdrive_save_error)\n",
    "                    additional_info.append(gdrive_save_error)\n",
    "            else:\n",
    "                gdrive_mount_fail_info = \"‚ùå Google Drive not mounted or mount failed. Cannot save.\"\n",
    "                print(gdrive_mount_fail_info)\n",
    "                additional_info.append(gdrive_mount_fail_info)\n",
    "        elif save_to_gdrive_val:\n",
    "            not_in_colab_info = \"‚ÑπÔ∏è 'Save to Google Drive' is only available in Google Colab.\"\n",
    "            print(not_in_colab_info)\n",
    "            additional_info.append(not_in_colab_info)\n",
    "        \n",
    "        info_text = f\"üå± Seed: {seed}\\nüïí Timestamp: {timestamp}\\nüîß Model: {current_model_id}\\nüé® Style: {style_name}\\nüìõ Filename: {base_filename}\"\n",
    "        full_info_text = info_text + \"\\n\" + \"\\n\".join(additional_info)\n",
    "        return image, full_info_text, gr.DownloadButton.update(value=local_img_path, visible=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        error_message = f\"‚ùå Error during generation: {str(e)}\"\n",
    "        print(error_message)\n",
    "        progress(1.0, desc=error_message)\n",
    "        return None, f\"{error_message}. Check Colab console.\", gr.DownloadButton.update(visible=False)\n",
    "\n",
    "print(\"Helper functions and generation logic defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Gradio User Interface\n",
    "\n",
    "This cell sets up the interactive web UI using Gradio. It includes input fields for prompts, style selection, sliders for generation parameters, seed control, and areas for displaying the generated image and status messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title 3. Define Gradio User Interface\n",
    "\n",
    "status_textbox_ref = [None] \n",
    "\n",
    "def create_gradio_ui_colab():\n",
    "    global status_textbox_ref\n",
    "\n",
    "    with gr.Blocks(theme=gr.themes.Soft(), css=\".gradio-container { max-width: 98% !important; } footer {display: none !important}\") as demo:\n",
    "        gr.Markdown(f\"# üé® Advanced Stable Diffusion UI (Colab)\")\n",
    "        status_textbox_ref[0] = gr.Textbox(label=\"Status\", value=\"Interface loaded. Please load a model using Cell 4.\", interactive=False, lines=2)\n",
    "\n",
    "        with gr.Row(equal_height=False):\n",
    "            with gr.Column(scale=2, min_width=400):\n",
    "                gr.Markdown(\"## ‚öôÔ∏è Input Controls\")\n",
    "                prompt_input = gr.Textbox(label=\"Enter your Prompt\", lines=3, placeholder=\"e.g., A majestic lion in a futuristic city, neon lights, detailed fur\")\n",
    "                negative_prompt_input = gr.Textbox(label=\"Negative Prompt (what to avoid)\", lines=2, placeholder=\"e.g., blurry, low quality, ugly, text, watermark, disfigured, cartoon\")\n",
    "                style_dropdown = gr.Dropdown(label=\"Artistic Style Preset\", choices=[\"None\"] + list(STYLE_PRESETS.keys())[1:], value=\"None\")\n",
    "\n",
    "                with gr.Row():\n",
    "                    inference_steps_slider = gr.Slider(minimum=10, maximum=100, value=25, step=1, label=\"Inference Steps\", info=\"More steps can improve detail but take longer.\")\n",
    "                    cfg_scale_slider = gr.Slider(minimum=1.0, maximum=20.0, value=7.5, step=0.1, label=\"CFG Scale (Guidance)\", info=\"How strongly the prompt guides the image.\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    seed_input = gr.Number(label=\"Seed\", value=get_random_seed(), precision=0, minimum=0)\n",
    "                    random_seed_button = gr.Button(\"üé≤ Randomize Seed\", scale=1, min_width=50)\n",
    "                \n",
    "                with gr.Accordion(\"Output Options\", open=True):\n",
    "                    custom_filename_input = gr.Textbox(label=\"Custom Filename Prefix (Optional)\", placeholder=\"my_creation_prefix\")\n",
    "                    save_to_gdrive_checkbox = gr.Checkbox(label=\"Save to Google Drive\", value=False, visible='google.colab' in sys.modules, info=f\"Saves to {GDRIVE_SAVE_PATH} if Drive is mounted.\")\n",
    "                    if 'google.colab' not in sys.modules:\n",
    "                        gr.Markdown(\"_(Google Drive option only available in Colab environment.)_\")\n",
    "\n",
    "                generate_button = gr.Button(\"üñºÔ∏è Generate Image\", variant=\"primary\")\n",
    "\n",
    "            with gr.Column(scale=3, min_width=520):\n",
    "                gr.Markdown(\"## üñºÔ∏è Generated Image\")\n",
    "                image_output = gr.Image(label=\"Output Image\", type=\"pil\", height=512, show_label=False, show_download_button=False, visible=False)\n",
    "                download_button = gr.DownloadButton(label=\"üíæ Download Image\", visible=False)\n",
    "                info_output = gr.Textbox(label=\"Generation Info & Logs\", lines=5, interactive=False)\n",
    "\n",
    "        def on_generate_wrapper(prompt, neg_prompt, style, steps, cfg, seed, filename_prefix, save_gdrive, progress=gr.Progress(track_ œÑœåœÑŒµ=True)):\n",
    "            yield {\n",
    "                generate_button: gr.update(interactive=False, value=\"‚è≥ Generating...\"),\n",
    "                status_textbox_ref[0]: gr.update(value=\"‚è≥ Generating image...\"),\n",
    "                image_output: gr.update(visible=False, value=None),\n",
    "                download_button: gr.update(visible=False),\n",
    "                info_output: \"\"\n",
    "            }\n",
    "            \n",
    "            img, info_text, dl_button_update = generate_image_colab_fn(prompt, neg_prompt, style, steps, cfg, seed, filename_prefix, save_gdrive, progress=progress)\n",
    "            \n",
    "            yield {\n",
    "                image_output: gr.update(value=img, visible=True if img else False),\n",
    "                info_output: info_text,\n",
    "                generate_button: gr.update(interactive=True, value=\"üñºÔ∏è Generate Image\"),\n",
    "                status_textbox_ref[0]: gr.update(value=\"‚úÖ Generation complete.\" if img else \"‚ùå Generation failed. Check console.\"),\n",
    "                download_button: dl_button_update\n",
    "            }\n",
    "\n",
    "        generate_button.click(\n",
    "            fn=on_generate_wrapper,\n",
    "            inputs=[prompt_input, negative_prompt_input, style_dropdown, inference_steps_slider, cfg_scale_slider, seed_input, custom_filename_input, save_to_gdrive_checkbox],\n",
    "            outputs=[generate_button, status_textbox_ref[0], image_output, download_button, info_output]\n",
    "        )\n",
    "\n",
    "        random_seed_button.click(fn=get_random_seed, inputs=None, outputs=seed_input)\n",
    "        \n",
    "        gr.Markdown(\"--- \")\n",
    "        gr.Markdown(\"**Note:** If you encounter issues, check the Colab console output for error messages. \"\n",
    "                    \"Ensure you have selected a GPU runtime (`Runtime` -> `Change runtime type` -> `GPU`).\")\n",
    "        gr.Markdown(\"--- \")\n",
    "        gr.Markdown(\"### Running Locally with `app.py`:\\n\"\n",
    "                    \"1. Download `app.py` and `requirements.txt` (if provided, or install: `pip install gradio torch diffusers transformers accelerate Pillow`).\\n\"\n",
    "                    \"2. Ensure you have Python and pip installed.\\n\"\n",
    "                    \"3. If using a GPU, ensure CUDA drivers and PyTorch with CUDA support are installed.\\n\"\n",
    "                    \"4. Open your terminal or command prompt, navigate to the directory where you saved `app.py`.\\n\"\n",
    "                    \"5. Run the app: `gradio app.py` or `python app.py`.\\n\"\n",
    "                    \"   You can also pass arguments, e.g., `python app.py --model_id stabilityai/stable-diffusion-2-1-base --port 7861`.\")\n",
    "\n",
    "    return demo\n",
    "\n",
    "gradio_app_instance = create_gradio_ui_colab()\n",
    "print(\"Gradio UI defined. Status textbox referenced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the Stable Diffusion Model\n",
    "\n",
    "This cell will download and load the pre-trained Stable Diffusion model. This can take several minutes, especially the first time you run it, as it needs to download the model weights (several gigabytes).\n",
    "\n",
    "**Choose your model below.** `runwayml/stable-diffusion-v1-5` is recommended for the free Colab tier (T4 GPU). SDXL models like `stabilityai/sdxl-base-1.0` are higher quality but require more VRAM (may not fit on T4, might need Colab Pro with A100/V100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title 4. Load the Model (can take a few minutes)\n",
    "\n",
    "# --- Model Configuration ---\n",
    "MODEL_TO_LOAD = \"runwayml/stable-diffusion-v1-5\"  # @param [\"runwayml/stable-diffusion-v1-5\", \"stabilityai/sdxl-base-1.0\", \"dreamlike-art/dreamlike-photoreal-2.0\", \"prompthero/openjourney\"] {\"allow-input\": true}\n",
    "USE_FLOAT16_PRECISION = True  # @param {type:\"boolean\"}\n",
    "ENABLE_ATTENTION_SLICING = True  # @param {type:\"boolean\"}\n",
    "\n",
    "print(f\"Selected model for loading: {MODEL_TO_LOAD}\")\n",
    "print(f\"Using float16 precision: {USE_FLOAT16_PRECISION}\")\n",
    "print(f\"Enabling attention slicing: {ENABLE_ATTENTION_SLICING}\")\n",
    "\n",
    "# Clean up any existing pipe to free memory before loading a new one\n",
    "if 'pipe' in globals() and pipe is not None:\n",
    "    print(\"Clearing existing model from memory...\")\n",
    "    del pipe\n",
    "    pipe = None # Ensure it's None\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Existing model cleared.\")\n",
    "\n",
    "# Update the global current_model_id for the info display\n",
    "current_model_id = MODEL_TO_LOAD\n",
    "\n",
    "# The load_model_colab is a generator. We iterate through its yields to process them.\n",
    "# The status updates are handled by yielding gr.update() for the status_textbox.\n",
    "if status_textbox_ref[0] is not None:\n",
    "    # This is tricky. Direct update of status_textbox_ref[0] from here won't reflect in UI\n",
    "    # unless this cell's execution is part of a Gradio event chain that has status_textbox_ref[0] as an output.\n",
    "    # For a standalone cell execution, print() is the most reliable feedback during the process.\n",
    "    # The yielded gr.update() values are for when this function is called *by* Gradio.\n",
    "    # We will manually set the value of the referenced textbox at the end of this cell.\n",
    "    status_textbox_ref[0].value = f\"Initiating load for {MODEL_TO_LOAD}...\" # Initial message\n",
    "    print(f\"UI Status Check: Textbox for status is available. Initial message set.\")\n",
    "\n",
    "    final_status_message = f\"Model {MODEL_TO_LOAD} loading process initiated.\"\n",
    "    # Consume the generator to execute the loading process\n",
    "    for status_update_yield in load_model_colab(MODEL_TO_LOAD, USE_FLOAT16_PRECISION, ENABLE_ATTENTION_SLICING, status_textbox_ref):\n",
    "        # The yielded gr.update object contains the value for the status textbox\n",
    "        if isinstance(status_update_yield, gr. uomini.Update):\n",
    "            final_status_message = status_update_yield.value\n",
    "            # We can't apply this update to the UI directly from here in a way that Gradio recognizes for a live update.\n",
    "            # Print it to console for confirmation.\n",
    "            print(f\"Loading progress: {final_status_message}\")\n",
    "        else:\n",
    "            # This case should not happen if load_model_colab always yields gr.update\n",
    "            print(f\"Unexpected yield from load_model_colab: {status_update_yield}\")\n",
    "    \n",
    "    # After the loop, set the final status on the referenced textbox\n",
    "    if status_textbox_ref[0] is not None:\n",
    "        status_textbox_ref[0].value = final_status_message # Manually update the value property\n",
    "        print(f\"Final status after loading attempt: {final_status_message}\")\n",
    "    \n",
    "    if pipe is None:\n",
    "        print(f\"Critical: Model {MODEL_TO_LOAD} FAILED to load. Check console output above for errors.\")\n",
    "        if status_textbox_ref[0] is not None: status_textbox_ref[0].value = f\"‚ùå Model {MODEL_TO_LOAD} FAILED to load. Check console.\"\n",
    "    else:\n",
    "        print(f\"Success: Model {MODEL_TO_LOAD} loaded and ready.\")\n",
    "        if status_textbox_ref[0] is not None: status_textbox_ref[0].value = f\"‚úÖ Model '{MODEL_TO_LOAD}' loaded. Ready to generate.\"\n",
    "else:\n",
    "    print(\"Warning: Gradio status textbox reference not found. Loading model without live UI status updates during load.\")\n",
    "    # Fallback if status_textbox_ref[0] isn't correctly captured (should not happen with current setup)\n",
    "    for _ in load_model_colab(MODEL_TO_LOAD, USE_FLOAT16_PRECISION, ENABLE_ATTENTION_SLICING, None): pass # Consume generator\n",
    "    if pipe is None: print(f\"Critical: Model {MODEL_TO_LOAD} FAILED to load (no UI ref). Check console.\")\n",
    "    else: print(f\"Success: Model {MODEL_TO_LOAD} loaded (no UI ref). Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Launch the Gradio App\n",
    "\n",
    "Run the cell below to start the Gradio interface. \n",
    "\n",
    "**Important:** \n",
    "*   Make sure **Cell 4 (Load the Model)** has completed successfully before running this cell.\n",
    "*   Click the **public URL** (it usually looks like `https://xxxx.gradio.live` or `https://xxxxxx.gradio.app`) that appears in the output to open the web UI in a new tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title 5. Launch Gradio UI\n",
    "if pipe is not None and gradio_app_instance is not None:\n",
    "    final_launch_message = f\"üöÄ Launching Gradio app with model: {current_model_id}\"\n",
    "    print(final_launch_message)\n",
    "    if status_textbox_ref[0] is not None:\n",
    "        status_textbox_ref[0].value = final_launch_message # Update status before launching\n",
    "    \n",
    "    # Clean up temp directory from previous runs if any\n",
    "    temp_image_dir = \"temp_generated_colab_images\"\n",
    "    if os.path.exists(temp_image_dir):\n",
    "        try:\n",
    "            import shutil\n",
    "            shutil.rmtree(temp_image_dir)\n",
    "            print(f\"Cleaned up old temp directory: {temp_image_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not clean up temp directory {temp_image_dir}: {e}\")\n",
    "    os.makedirs(temp_image_dir, exist_ok=True) # Ensure it exists for current session\n",
    "\n",
    "    gradio_app_instance.queue().launch(debug=False, share=True) # share=True creates a public link\n",
    "else:\n",
    "    error_msg = \"ERROR: Model not loaded or Gradio UI not defined. Cannot launch Gradio app.\"\n",
    "    print(error_msg)\n",
    "    if status_textbox_ref[0] is not None:\n",
    "        status_textbox_ref[0].value = error_msg\n",
    "    print(\"Please ensure Cell 3 (Define UI) and Cell 4 (Load Model) executed successfully.\")\n",
    "    if pipe is None: print(\"- Model (pipe) is None.\")\n",
    "    if gradio_app_instance is None: print(\"- Gradio App (gradio_app_instance) is None.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
