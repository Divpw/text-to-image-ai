{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ultra-Professional Stable Diffusion Image Generation System\n",
    "\n",
    "This notebook sets up an advanced image generation system with Stable Diffusion (including SDXL), Real-ESRGAN upscaling, and a refined Gradio UI. It integrates with a GitHub repository to ensure you're running the latest code.\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Set Your GitHub Repo URL (Important!):** In the first code cell (Cell 1), update the `GITHUB_REPO_URL` variable to point to **your fork** or the main project repository if you intend to just pull updates.\n",
    "2.  **Enable GPU:** Go to `Runtime` -> `Change runtime type` and select `GPU` (e.g., T4) as the hardware accelerator.\n",
    "3.  **Run Cells Sequentially:** Execute each cell in order from top to bottom.\n",
    "    *   **Cell 1 (Setup):** Clones/pulls the GitHub repo and sets up the environment.\n",
    "    *   **Cell 2 (Install Dependencies):** Installs necessary Python libraries.\n",
    "    *   **Subsequent Cells:** Import libraries, define UI, load models, and launch the Gradio app.\n",
    "    *   Pay attention to console outputs for status and potential errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1. Setup Environment & Sync with GitHub Repository\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- GitHub Repository Configuration ---\n",
    "# IMPORTANT: Replace this with the actual URL of your GitHub repository\n",
    "GITHUB_REPO_URL = \"https://github.com/YOUR_USERNAME/YOUR_REPO.git\" \n",
    "REPO_DIR_NAME = \"AI_Art_Repo\" # Name of the directory to clone into\n",
    "PROJECT_ROOT = os.path.join(\"/content\", REPO_DIR_NAME)\n",
    "\n",
    "print(f\"Project files will be synced/placed in: {PROJECT_ROOT}\")\n",
    "\n",
    "# --- Clone or Pull Repository ---\n",
    "if not os.path.exists(PROJECT_ROOT):\n",
    "    print(f\"Cloning repository from {GITHUB_REPO_URL} into {PROJECT_ROOT}...\")\n",
    "    # Perform a shallow clone initially if you only need the latest version of the main branch\n",
    "    !git clone --depth 1 --branch main {GITHUB_REPO_URL} {PROJECT_ROOT}\n",
    "    print(\"Repository cloned.\")\n",
    "else:\n",
    "    print(f\"Repository directory {PROJECT_ROOT} already exists. Performing git pull...\")\n",
    "    os.chdir(PROJECT_ROOT) # Change current directory to the repo path\n",
    "    !git fetch origin main # Fetch the latest changes from the main branch\n",
    "    !git reset --hard origin/main # Force reset to the latest version of main (overwrites local changes)\n",
    "    # Alternative: !git pull origin main # This might lead to merge conflicts if local changes exist\n",
    "    print(\"Repository updated with git pull (hard reset to origin/main).\")\n",
    "    os.chdir(\"/content/\") # Change back to /content to avoid issues with subsequent cells assuming this path\n",
    "\n",
    "# --- Add Project Root to Python Path ---\n",
    "# This allows importing .py files (like app.py or utils.py) if they are in the repo\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "    print(f\"Added {PROJECT_ROOT} to Python sys.path.\")\n",
    "else:\n",
    "    print(f\"{PROJECT_ROOT} is already in Python sys.path.\")\n",
    "\n",
    "print(\"Environment setup and GitHub sync complete.\")\n",
    "# Note: If your Gradio app (`app.py`) or other scripts are intended to be run *from* the notebook,\n",
    "# ensure their paths are relative to PROJECT_ROOT or use absolute paths starting with PROJECT_ROOT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title 2. Install Dependencies\n",
    "# Base dependencies for Stable Diffusion, Gradio, and utilities\n",
    "!pip install diffusers transformers accelerate gradio bitsandbytes Pillow --quiet\n",
    "\n",
    "# Dependencies for Real-ESRGAN upscaling\n",
    "# basicsr is a dependency for realesrgan\n",
    "!pip install basicsr realesrgan --quiet\n",
    "\n",
    "print(\"All dependencies, including for Real-ESRGAN, installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries, Define Presets, Helpers, and Generation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title 2. Imports, Presets, Helpers, and Generation Logic\n",
    "import gradio as gr\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler, AutoPipelineForText2Image\n",
    "from PIL import Image\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import sys # For checking if in Colab\n",
    "import traceback # For detailed error logging\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# --- Global Variables ---\n",
    "pipe = None\n",
    "current_model_id = None # Will be set by the load_model_colab function\n",
    "loaded_model_type = None # To track if 'sdxl', 'sd1.5', or 'other'\n",
    "\n",
    "# --- Model Configuration (to be set in Cell 4 before loading) ---\n",
    "PREFERRED_SDXL_MODEL_ID = \"stabilityai/sdxl-base-1.0\"\n",
    "FALLBACK_SD15_MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# --- Style Presets (can be expanded) ---\n",
    "STYLE_PRESETS = {\n",
    "    \"None\": {\"prompt_suffix\": \"\", \"negative_prompt_prefix\": \"\"},\n",
    "    \"Realistic\": {\"prompt_suffix\": \"photorealistic, 4k, ultra detailed, cinematic lighting, professional photography\", \"negative_prompt_prefix\": \"cartoon, anime, drawing, sketch, stylized, illustration, painting, art, text, watermark, signature, ugly, deformed\"},\n",
    "    \"Cyberpunk\": {\"prompt_suffix\": \"cyberpunk cityscape, neon lights, futuristic, dystopian, highly detailed, intricate\", \"negative_prompt_prefix\": \"historical, medieval, nature, daytime, sunny, ugly, deformed\"},\n",
    "    \"Anime\": {\"prompt_suffix\": \"anime style, key visual, vibrant, beautiful, detailed illustration, official art, studio ghibli, makoto shinkai\", \"negative_prompt_prefix\": \"photorealistic, 3d render, ugly, disfigured, real life, text, watermark\"},\n",
    "    \"Watercolor\": {\"prompt_suffix\": \"watercolor painting, soft wash, wet-on-wet, flowing colors, detailed brushstrokes\", \"negative_prompt_prefix\": \"photorealistic, harsh lines, 3d render, octane render, text, signature\"},\n",
    "    \"3D Render\": {\"prompt_suffix\": \"3d render, octane render, blender, vray, detailed textures, physically based rendering, cinematic\", \"negative_prompt_prefix\": \"2d, drawing, sketch, painting, illustration, flat, cartoon, text\"},\n",
    "}\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def get_random_seed():\n",
    "    return random.randint(0, 2**32 - 1)\n",
    "\n",
    "def parse_dimensions(dim_string): # e.g., \"512x768\"\n",
    "    try:\n",
    "        w, h = map(int, dim_string.split('x'))\n",
    "        return w, h\n",
    "    except:\n",
    "        print(f\"Warning: Could not parse dimensions '{dim_string}'. Defaulting to 512x512.\")\n",
    "        return 512, 512 # Default if parsing fails\n",
    "\n",
    "# --- Real-ESRGAN Upscaler Setup ---\n",
    "from realesrgan import RealESRGANer\n",
    "from basicsr.archs.rrdbnet_arch import RRDBNet\n",
    "import numpy as np\n",
    "import cv2 # OpenCV for image handling with RealESRGAN\n",
    "\n",
    "upscaler_model = None\n",
    "REALESRGAN_MODEL_NAME = 'RealESRGAN_x4plus' # General purpose model\n",
    "# REALESRGAN_MODEL_NAME = 'RealESRGAN_x4plus_anime_6B' # For anime, larger model\n",
    "REALESRGAN_SCALE = 4\n",
    "\n",
    "def load_upscaler_model(status_gr_textbox_ref=None):\n",
    "    global upscaler_model\n",
    "    if upscaler_model is not None:\n",
    "        print(\"Upscaler model already loaded.\")\n",
    "        if status_gr_textbox_ref and status_gr_textbox_ref[0] is not None:\n",
    "            # This is tricky to update live from a non-event. Best effort.\n",
    "            current_status = status_gr_textbox_ref[0].value\n",
    "            status_gr_textbox_ref[0].value = f\"{current_status}\\nUpscaler model already loaded.\"\n",
    "        return True\n",
    "\n",
    "    print(f\"Loading Real-ESRGAN upscaler model: {REALESRGAN_MODEL_NAME}...\")\n",
    "    if status_gr_textbox_ref and status_gr_textbox_ref[0] is not None:\n",
    "        current_status = status_gr_textbox_ref[0].value\n",
    "        status_gr_textbox_ref[0].value = f\"{current_status}\\nLoading Real-ESRGAN upscaler ({REALESRGAN_MODEL_NAME})...\"\n",
    "        \n",
    "    try:\n",
    "        # Determine model path. RealESRGANer usually handles downloads for known models.\n",
    "        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=REALESRGAN_SCALE)\n",
    "        # Use half precision if on CUDA for RealESRGANer as well\n",
    "        half_precision = torch.cuda.is_available() # and use_float16_for_main_model perhaps\n",
    "\n",
    "        upscaler_model = RealESRGANer(\n",
    "            scale=REALESRGAN_SCALE,\n",
    "            model_path=None, # Let RealESRGANer download the model if model_name is provided\n",
    "            model=model, # Pass the RRDBNet instance\n",
    "            dni_weight=None, # Not using DNI for general purpose\n",
    "            model_name=REALESRGAN_MODEL_NAME, # This tells RealESRGANer which weights to download\n",
    "            tile=0, # Tile size for processing, 0 for auto\n",
    "            tile_pad=10,\n",
    "            pre_pad=0,\n",
    "            half=half_precision, # Use half precision on GPU\n",
    "            gpu_id=0 if torch.cuda.is_available() else None\n",
    "        )\n",
    "        print(f\"Real-ESRGAN upscaler model '{REALESRGAN_MODEL_NAME}' loaded successfully.\")\n",
    "        if status_gr_textbox_ref and status_gr_textbox_ref[0] is not None:\n",
    "            current_status = status_gr_textbox_ref[0].value\n",
    "            status_gr_textbox_ref[0].value = f\"{current_status}\\nUpscaler model loaded.\"\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Real-ESRGAN upscaler model: {e}\")\n",
    "        traceback.print_exc()\n",
    "        upscaler_model = None\n",
    "        if status_gr_textbox_ref and status_gr_textbox_ref[0] is not None:\n",
    "            current_status = status_gr_textbox_ref[0].value\n",
    "            status_gr_textbox_ref[0].value = f\"{current_status}\\nERROR loading upscaler model. Upscaling disabled.\"\n",
    "        return False\n",
    "\n",
    "def upscale_image_with_realesrgan(pil_image, progress_callback=None):\n",
    "    global upscaler_model\n",
    "    if upscaler_model is None:\n",
    "        print(\"Upscaler model not loaded. Skipping upscale.\")\n",
    "        return pil_image # Return original image\n",
    "\n",
    "    print(f\"Upscaling image (original size: {pil_image.size[0]}x{pil_image.size[1]})...\")\n",
    "    if progress_callback: progress_callback(0.1, desc=\"🖼️ Upscaling (Real-ESRGAN)...
")
    "        traceback.print_exc() # Print SDXL error details to console\n",
    "        pipe = None # Ensure pipe is reset\n",
    "\n",
    "    # If SDXL fails, attempt to load SD 1.5 fallback\n",
    "    for status_update in _update_status_and_yield(f\"Attempting to load fallback SD 1.5 model: {fallback_sd15_id}...\"):\n",
    "        if status_update: yield status_update\n",
    "    try:\n",
    "        # For SD 1.5, explicitly use StableDiffusionPipeline and EulerDiscreteScheduler for robustness\n",
    "        scheduler = EulerDiscreteScheduler.from_pretrained(fallback_sd15_id, subfolder=\"scheduler\")\n",
    "        sd15_pipeline_args = {**pipeline_args, \"scheduler\": scheduler}\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(fallback_sd15_id, **sd15_pipeline_args)\n",
    "        current_model_id = fallback_sd15_id\n",
    "        loaded_model_type = \"sd1.5\"\n",
    "        if torch.cuda.is_available(): pipe.to(\"cuda\")\n",
    "        if use_attention_slicing and hasattr(pipe, \"enable_attention_slicing\"): pipe.enable_attention_slicing()\n",
    "        for status_update in _update_status_and_yield(f\"Successfully loaded fallback SD 1.5 model: {current_model_id}\"):\n",
    "            if status_update: yield status_update\n",
    "        return\n",
    "    except Exception as e_sd15:\n",
    "        for status_update in _update_status_and_yield(f\"Failed to load fallback SD 1.5 model '{fallback_sd15_id}': {e_sd15}. Model loading failed.\"):\n",
    "            if status_update: yield status_update\n",
    "        traceback.print_exc() # Print SD1.5 error details\n",
    "        pipe = None\n",
    "        current_model_id = None\n",
    "        loaded_model_type = None\n",
    "        return\n",
    "\n",
    "# --- Image Generation Function ---\n",
    "def generate_image_colab_fn(prompt, negative_prompt, style_name, dimensions_str, \n",
    "                            num_inference_steps, guidance_scale, seed_value, \n",
    "                            custom_filename_prefix_val, save_to_gdrive_val, \n",
    "                            upscale_active_val, # New parameter for upscaling\n",
    "                            progress=gr.Progress(track_ τότε=True)):\n",
    "    global pipe, current_model_id, loaded_model_type, GDRIVE_MOUNTED_SUCCESSFULLY, GDRIVE_SAVE_PATH, upscaler_model\n",
    "    \n",
    "    additional_info = []\n",
    "    if pipe is None:\n",
    "        return None, \"Model not loaded. Please run the model loading cell first.\", gr.DownloadButton.update(visible=False)\n",
    "\n",
    "    progress(0, desc=\"🎨 Starting generation...\")\n",
    "    width, height = parse_dimensions(dimensions_str)\n",
    "    additional_info.append(f\"Requested dimensions: {width}x{height}\")\n",
    "\n",
    "    # Dimension compatibility check (simple version)\n",
    "    if loaded_model_type == \"sd1.5\" and (width > 768 or height > 768):\n",
    "        additional_info.append(f\"Warning: SD 1.5 works best at 512x512 or 768x768. Requested: {width}x{height}. Adjusting to 512x512 for stability.\")\n",
    "        width, height = 512, 512\n",
    "    elif loaded_model_type == \"sdxl\" and (width < 768 or height < 768):\n",
    "         additional_info.append(f\"Warning: SDXL typically performs better at 1024x1024 or 768x768. Requested: {width}x{height}. Results may vary.\")\n",
    "    # On free tier Colab (T4), 1024x1024 for SDXL might be very slow or cause OOM.\n",
    "    # Consider adding a check for GPU type if more fine-grained control is needed.\n",
    "\n",
    "    styled_prompt, style_negative_prefix = apply_style(prompt, style_name)\n",
    "    final_negative_prompt = negative_prompt\n",
    "    if style_negative_prefix:\n",
    "        final_negative_prompt = f\"{style_negative_prefix}, {negative_prompt}\" if negative_prompt else style_negative_prefix\n",
    "\n",
    "    progress(0.1, desc=f\"📝 Prompt: {styled_prompt[:100]}...\")\n",
    "    print(f\"Generating with Prompt: '{styled_prompt}' at {width}x{height}\")\n",
    "    if final_negative_prompt: print(f\"Negative Prompt: '{final_negative_prompt}'\")\n",
    "    \n",
    "    try:\n",
    "        seed = int(seed_value)\n",
    "        if seed == -1: seed = get_random_seed()\n",
    "    except (ValueError, TypeError):\n",
    "        seed = get_random_seed()\n",
    "    print(f\"🌱 Seed: {seed}\")\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    generator = torch.Generator(device).manual_seed(seed)\n",
    "    num_inference_steps = int(num_inference_steps)\n",
    "    guidance_scale = float(guidance_scale)\n",
    "\n",
    "    generation_args = {\n",
    "        \"prompt\": styled_prompt,\n",
    "        \"negative_prompt\": final_negative_prompt if final_negative_prompt else None,\n",
    "        \"num_inference_steps\": num_inference_steps,\n",
    "        \"guidance_scale\": guidance_scale,\n",
    "        \"generator\": generator,\n",
    "        \"width\": width,\n",
    "        \"height\": height\n",
    "    }\n",
    "\n",
    "    image = None\n",
    "    try:\n",
    "        with torch.autocast(\"cuda\", enabled=torch.cuda.is_available()): # Handles float16 context\n",
    "            # Simulate progress for the diffusion steps\n",
    "            for i in range(num_inference_steps):\n",
    "                progress(i / num_inference_steps, desc=\"🌀 Diffusing...\")\n",
    "                if i == num_inference_steps - 1: # Actual generation on the last conceptual step\n",
    "                    image = pipe(**generation_args).images[0]\n",
    "            if image is None and num_inference_steps == 0 : # Handle edge case of 0 steps if pipe allows (though slider min is 10)\n",
    "                 image = pipe(**generation_args).images[0] # if steps = 0, make at least one call\n",
    "        \n",
    "        if image is None: raise RuntimeError(\"Image generation failed to produce an output.\")\n",
    "\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        sane_prompt_words = \"\".join(c if c.isalnum() or c.isspace() else \" \" for c in styled_prompt).split()\n",
    "        prompt_filename_prefix = \"_\".join(sane_prompt_words[:5])[:30]\n",
    "        if not prompt_filename_prefix: prompt_filename_prefix = \"generated_image\"\n",
    "\n",
    "        filename_prefix_to_use = custom_filename_prefix_val.strip() if custom_filename_prefix_val.strip() else prompt_filename_prefix\n",
    "        base_filename = f\"{filename_prefix_to_use}_{width}x{height}_{seed}_{timestamp}.png\"\n",
    "\n",
    "        print(f\"✅ Initial image generation successful. Filename: {base_filename}\")\n",
    "        additional_info.append(f\"Initial image: {width}x{height}\")\n",
    "        progress(1.0, desc=\"🎉 Initial image ready! Checking for upscale...\")\n",
    "        \n",
    "        # Upscaling Step\n",
    "        if upscale_active_val:\n",
    "            if upscaler_model is None: # Attempt to load upscaler if not already loaded\n",
    "                print(\"Upscaler not loaded, attempting to load now for upscaling...\")\n",
    "                # status_gr_textbox_ref is not directly available here, rely on console for this ad-hoc load\n",
    "                load_upscaler_model() \n",
    "            \n",
    "            if upscaler_model is not None:\n",
    "                additional_info.append(\"Upscaling enabled...\")\n",
    "                image = upscale_image_with_realesrgan(image, progress_callback=progress) # Pass progress object\n",
    "                upscaled_w, upscaled_h = image.size\n",
    "                additional_info.append(f\"Image upscaled to: {upscaled_w}x{upscaled_h}\")\n",
    "                base_filename = f\"{filename_prefix_to_use}_{upscaled_w}x{upscaled_h}_upscaled_{seed}_{timestamp}.png\"\n",
    "                print(f\"🖼️ Image upscaled. New filename: {base_filename}\")\n",
    "                progress(1.0, desc=\"🎉 Upscaling Complete!\")\n",
    "            else:\n",
    "                additional_info.append(\"❌ Upscaling skipped: Upscaler model failed to load.\")\n",
    "                print(\"Upscaling skipped as upscaler model is not available.\")\n",
    "        else:\n",
    "            additional_info.append(\"Upscaling disabled.\")\n",
    "\n",
    "        temp_dir = \"temp_generated_colab_images\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        local_img_path = os.path.join(temp_dir, base_filename)\n",
    "        image.save(local_img_path)\n",
    "        additional_info.append(f\"Saved locally for download: {local_img_path}\")\n",
    "\n",
    "        if save_to_gdrive_val and 'google.colab' in sys.modules:\n",
    "            if not GDRIVE_MOUNTED_SUCCESSFULLY: GDRIVE_MOUNTED_SUCCESSFULLY = mount_google_drive()\n",
    "            if GDRIVE_MOUNTED_SUCCESSFULLY:\n",
    "                try:\n",
    "                    os.makedirs(GDRIVE_SAVE_PATH, exist_ok=True)\n",
    "                    gdrive_img_path = os.path.join(GDRIVE_SAVE_PATH, base_filename)\n",
    "                    image.save(gdrive_img_path)\n",
    "                    additional_info.append(f\"Saved to GDrive: {gdrive_img_path}\")\n",
    "                except Exception as e_gdrive:\n",
    "                    additional_info.append(f\"❌ Error saving to Google Drive: {e_gdrive}\")\n",
    "            else:\n",
    "                additional_info.append(\"❌ Google Drive not mounted. Cannot save.\")\n",
    "        elif save_to_gdrive_val:\n",
    "            additional_info.append(\"ℹ️ 'Save to Google Drive' is Colab-only.\")\n",
    "        \n",
    "        img_w, img_h = image.size\n",
    "        info_text = f\"🖼️ Final Size: {img_w}x{img_h}\\n🌱 Seed: {seed}\\n🕒 Timestamp: {timestamp}\\n🔧 Model: {current_model_id} ({loaded_model_type})\\n🎨 Style: {style_name}\\n📛 Filename: {base_filename}\"\n",
    "        full_info_text = info_text + \"\\n\\n**Logs:**\\n\" + \"\\n\".join(additional_info)\n",
    "        return image, full_info_text, gr.DownloadButton.update(value=local_img_path, visible=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        error_message = f\"❌ Error during generation/upscaling: {str(e)}\"\n",
    "        print(error_message)\n",
    "        progress(1.0, desc=error_message)\n",
    "        return None, f\"{error_message}. Check Colab console.\", gr.DownloadButton.update(visible=False)\n",
    "\n",
    "print(\"Helper functions and generation logic defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Gradio User Interface\n",
    "\n",
    "This cell sets up the interactive web UI using Gradio. It includes input fields for prompts, style selection, sliders for generation parameters, seed control, and areas for displaying the generated image and status messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title 3. Define Gradio User Interface\n",
    "\n",
    "status_textbox_ref = [None] \n",
    "\n",
    "def create_gradio_ui_colab():\n",
    "    global status_textbox_ref\n",
    "    dimension_choices = [\"512x512\", \"512x768\", \"768x512\", \"768x768\", \"1024x1024\", \"768x1024\", \"1024x768\"]\n",
    "    default_dimension = \"512x512\"\n",
    "\n",
    "    with gr.Blocks(theme=gr.themes.Soft(primary_hue=gr.themes.colors.blue, secondary_hue=gr.themes.colors.sky), css=\".gradio-container { max-width: 98% !important; padding: 15px; } footer {display: none !important} .gr-panel {border-radius: 10px !important; box-shadow: 0 2px 5px rgba(0,0,0,0.1) !important; padding:15px !important;}\") as demo:\n",
    "        gr.Markdown(f\"# ✨ Ultra-Professional Stable Diffusion UI ✨\")\n",
    "        status_textbox_ref[0] = gr.Textbox(label=\"Status\", value=\"Interface loaded. Please load diffusion model (Cell 4) & upscaler (Cell 5).\", interactive=False, lines=2)\n",
    "\n",
    "        with gr.Row(equal_height=False):\n",
    "            with gr.Column(scale=2, min_width=480, elem_classes=\"gr-panel\"):\n",
    "                gr.Markdown(\"## ⚙️ Input Controls\")\n",
    "                with gr.Group():\n",
    "                    prompt_input = gr.Textbox(label=\"Enter your Prompt\", lines=3, placeholder=\"e.g., A majestic lion in a futuristic city, neon lights, detailed fur\")\n",
    "                    negative_prompt_input = gr.Textbox(label=\"Negative Prompt (what to avoid)\", lines=2, placeholder=\"e.g., blurry, low quality, ugly, text, watermark, disfigured, cartoon\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=2):\n",
    "                        style_dropdown = gr.Dropdown(label=\"Artistic Style Preset\", choices=[\"None\"] + list(STYLE_PRESETS.keys()), value=\"None\")\n",
    "                    with gr.Column(scale=3):\n",
    "                        dimensions_dropdown = gr.Dropdown(label=\"Image Dimensions (W x H)\", choices=dimension_choices, value=default_dimension, \n",
    "                                                        info=\"SD1.5 best at 512/768. SDXL best at 1024.\")\n",
    "                with gr.Row():\n",
    "                    inference_steps_slider = gr.Slider(minimum=10, maximum=50, value=25, step=1, label=\"Inference Steps\", info=\"Usually 20-40.\")\n",
    "                    cfg_scale_slider = gr.Slider(minimum=1.0, maximum=15.0, value=7.0, step=0.5, label=\"CFG Scale\", info=\"5-10 typical.\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    seed_input = gr.Number(label=\"Seed (-1 for random)\", value=-1, precision=0)\n",
    "                    random_seed_button = gr.Button(\"🎲 Randomize\", scale=1, min_width=50)\n",
    "                \n",
    "                with gr.Accordion(\"Output & Advanced Options\", open=True):\n",
    "                    custom_filename_input = gr.Textbox(label=\"Custom Filename Prefix (Optional)\", placeholder=\"my_creation_prefix\")\n",
    "                    upscale_checkbox = gr.Checkbox(label=\"✨ Upscale Image (Real-ESRGAN x4)\", value=True, info=\"Increases generation time significantly.\")\n",
    "                    save_to_gdrive_checkbox = gr.Checkbox(label=\"💾 Save to Google Drive\", value=False, visible='google.colab' in sys.modules, info=f\"Saves to {GDRIVE_SAVE_PATH} if Drive mounted.\")\n",
    "                    if 'google.colab' not in sys.modules:\n",
    "                        gr.Markdown(\"_(Google Drive option only available in Colab.)_\")\n",
    "\n",
    "                generate_button = gr.Button(\"🖼️ Generate Image\", variant=\"primary\", elem_id=\"generate_button_main\")\n",
    "\n",
    "            with gr.Column(scale=3, min_width=520, elem_classes=\"gr-panel\"):\n",
    "                gr.Markdown(\"## 🖼️ Generated Image\")\n",
    "                # Use a group to manage visibility of image and button together, and for spinner\n",
    "                with gr.Group() as output_group:\n",
    "                    image_output = gr.Image(label=\"Output Image\", type=\"pil\", height=512, show_label=False, show_download_button=False, visible=False)\n",
    "                    download_button = gr.DownloadButton(label=\"📥 Download Image\", visible=False)\n",
    "                \n",
    "                info_output = gr.Textbox(label=\"Generation Info & Logs\", lines=8, interactive=False, max_lines=15)\n",
    "\n",
    "        def on_generate_wrapper(prompt, neg_prompt, style, dimensions, steps, cfg, seed, filename_prefix, save_gdrive, upscale_active, progress=gr.Progress(track_ τότε=True)):\n",
    "            # This will show a spinner over the output_group\n",
    "            yield {\n",
    "                generate_button: gr.update(interactive=False, value=\"⏳ Working...\"),\n",
    "                status_textbox_ref[0]: gr.update(value=\"⏳ Processing request...\"),\n",
    "                image_output: gr.update(visible=False, value=None),\n",
    "                download_button: gr.update(visible=False),\n",
    "                info_output: \"Starting generation process...\",\n",
    "                output_group: gr.update(visible=True) # Ensure group is visible for spinner\n",
    "            }\n",
    "            \n",
    "            img, info_text, dl_button_update = generate_image_colab_fn(prompt, neg_prompt, style, dimensions, steps, cfg, seed, filename_prefix, save_gdrive, upscale_active, progress=progress)\n",
    "            \n",
    "            yield {\n",
    "                image_output: gr.update(value=img, visible=True if img else False),\n",
    "                info_output: info_text,\n",
    "                generate_button: gr.update(interactive=True, value=\"🖼️ Generate Image\"),\n",
    "                status_textbox_ref[0]: gr.update(value=\"✅ Process complete.\" if img else \"❌ Process failed. Check logs & console.\"),\n",
    "                download_button: dl_button_update,\n",
    "            }\n",
    "\n",
    "        generate_button.click(\n",
    "            fn=on_generate_wrapper,\n",
    "            inputs=[\n",
    "                prompt_input, negative_prompt_input, style_dropdown, dimensions_dropdown, \n",
    "                inference_steps_slider, cfg_scale_slider, seed_input, \n",
    "                custom_filename_input, save_to_gdrive_checkbox, upscale_checkbox\n",
    "            ],\n",
    "            outputs=[generate_button, status_textbox_ref[0], image_output, download_button, info_output, output_group]\n",
    "        )\n",
    "\n",
    "        def random_seed_click():\n",
    "            return get_random_seed()\n",
    "        random_seed_button.click(fn=random_seed_click, inputs=None, outputs=seed_input)\n",
    "        \n",
    "        gr.Markdown(\"--- \")\n",
    "        gr.Markdown(\"**Note:** If you encounter issues, check the Colab console output for error messages. \"\n",
    "                    \"Ensure you have selected a GPU runtime (`Runtime` -> `Change runtime type` -> `GPU`).\")\n",
    "        gr.Markdown(\"### Running Locally with `app.py`:\\n\"\n",
    "                    \"1. Download `app.py` and `requirements.txt` (if provided, or install: `pip install gradio torch diffusers transformers accelerate Pillow basicsr realesrgan`).\\n\"\n",
    "                    \"2. Ensure you have Python and pip installed.\\n\"\n",
    "                    \"3. If using a GPU, ensure CUDA drivers and PyTorch with CUDA support are installed.\\n\"\n",
    "                    \"4. Open your terminal or command prompt, navigate to the directory where you saved `app.py`.\\n\"\n",
    "                    \"5. Run the app: `gradio app.py` or `python app.py`.\\n\"\n",
    "                    \"   You can also pass arguments, e.g., `python app.py --model_id stabilityai/stable-diffusion-2-1-base --port 7861`.\")\n",
    "\n",
    "    return demo\n",
    "\n",
    "gradio_app_instance = create_gradio_ui_colab()\n",
    "print(\"Gradio UI defined. Status textbox referenced.\")"
   ]
  },
  {
   "cell_type": "markdown",
    "    if style_name == \"None\" or style_name not in STYLE_PRESETS:\n",
    "        return prompt, \"\"\n",
    "    preset = STYLE_PRESETS[style_name]\n",
    "    styled_prompt = f\"{prompt.strip()}, {preset['prompt_suffix']}\" if prompt.strip() else preset['prompt_suffix']\n",
    "    return styled_prompt.strip(\", \"), preset['negative_prompt_prefix'].strip(\", \")\n",
    "\n",
    "# --- Google Drive Mounting ---\n",
    "def mount_google_drive():\n",
    "    global GDRIVE_MOUNTED_SUCCESSFULLY\n",
    "    if 'google.colab' in sys.modules:\n",
    "        try:\n",
    "            from google.colab import drive\n",
    "            drive.mount('/content/drive', force_remount=True) # Force remount can be useful\n",
    "            print(\"Google Drive mounted successfully at /content/drive\")\n",
    "            GDRIVE_MOUNTED_SUCCESSFULLY = True\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error mounting Google Drive: {e}\")\n",
    "            GDRIVE_MOUNTED_SUCCESSFULLY = False\n",
    "            return False\n",
    "    GDRIVE_MOUNTED_SUCCESSFULLY = False\n",
    "    return False\n",
    "GDRIVE_MOUNTED_SUCCESSFULLY = False\n",
    "GDRIVE_SAVE_PATH = \"/content/drive/MyDrive/AI_Generated_Images/UltraProfessionalSD/\"\n",
    "\n",
    "# --- Model Loading Function (SDXL with SD1.5 Fallback) ---\n",
    "def load_model_colab(preferred_sdxl_id, fallback_sd15_id, use_float16=True, use_attention_slicing=True, status_gr_textbox_ref=None):\n",
    "    global pipe, current_model_id, loaded_model_type\n",
    "\n",
    "    def _update_status_and_yield(message):\n",
    "        print(message)\n",
    "        if status_gr_textbox_ref and status_gr_textbox_ref[0] is not None:\n",
    "            # This yield is for Gradio to update the textbox if called within a Gradio event chain.\n",
    "            # For direct cell execution, the print statement is the primary feedback during the process.\n",
    "            # The final status is set on the textbox instance directly after the generator is consumed.\n",
    "            yield gr.update(value=message)\n",
    "        else:\n",
    "            yield None # Must yield something if it's a generator\n",
    "\n",
    "    # Attempt to load SDXL model first\n",
    "    for status_update in _update_status_and_yield(f\"Attempting to load preferred SDXL model: {preferred_sdxl_id}...\"):\n",
    "        if status_update: yield status_update\n",
    "    \n",
    "    pipeline_args = {\"torch_dtype\": torch.float16} if torch.cuda.is_available() and use_float16 else {}\n",
    "    if use_float16: print(\"Using float16 precision for model loading.\")\n",
    "\n",
    "    try:\n",
    "        pipe = AutoPipelineForText2Image.from_pretrained(preferred_sdxl_id, **pipeline_args)\n",
    "        current_model_id = preferred_sdxl_id\n",
    "        loaded_model_type = \"sdxl\"\n",
    "        if torch.cuda.is_available(): pipe.to(\"cuda\")\n",
    "        if use_attention_slicing and hasattr(pipe, \"enable_attention_slicing\"): pipe.enable_attention_slicing()\n",
    "        for status_update in _update_status_and_yield(f\"Successfully loaded SDXL model: {current_model_id}\"):\n",
    "            if status_update: yield status_update\n",
    "        return\n",
    "    except Exception as e_sdxl:\n",
    "        for status_update in _update_status_and_yield(f\"Failed to load SDXL model '{preferred_sdxl_id}': {e_sdxl}. Trying fallback SD 1.5 model...\"):\n",
    "            if status_update: yield status_update\n",
    "        traceback.print_exc() # Print SDXL error details to console\n",
    "        pipe = None # Ensure pipe is reset\n",
    "\n",
    "    # If SDXL fails, attempt to load SD 1.5 fallback\n",
    "    for status_update in _update_status_and_yield(f\"Attempting to load fallback SD 1.5 model: {fallback_sd15_id}...\"):\n",
    "        if status_update: yield status_update\n",
    "    try:\n",
    "        # For SD 1.5, explicitly use StableDiffusionPipeline and EulerDiscreteScheduler for robustness\n",
    "        scheduler = EulerDiscreteScheduler.from_pretrained(fallback_sd15_id, subfolder=\"scheduler\")\n",
    "        sd15_pipeline_args = {**pipeline_args, \"scheduler\": scheduler}\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(fallback_sd15_id, **sd15_pipeline_args)\n",
    "        current_model_id = fallback_sd15_id\n",
    "        loaded_model_type = \"sd1.5\"\n",
    "        if torch.cuda.is_available(): pipe.to(\"cuda\")\n",
    "        if use_attention_slicing and hasattr(pipe, \"enable_attention_slicing\"): pipe.enable_attention_slicing()\n",
    "        for status_update in _update_status_and_yield(f\"Successfully loaded fallback SD 1.5 model: {current_model_id}\"):\n",
    "            if status_update: yield status_update\n",
    "        return\n",
    "    except Exception as e_sd15:\n",
    "        for status_update in _update_status_and_yield(f\"Failed to load fallback SD 1.5 model '{fallback_sd15_id}': {e_sd15}. Model loading failed.\"):\n",
    "            if status_update: yield status_update\n",
    "        traceback.print_exc() # Print SD1.5 error details\n",
    "        pipe = None\n",
    "        current_model_id = None\n",
    "        loaded_model_type = None\n",
    "        return\n",
    "\n",
    "# --- Image Generation Function ---\n",
    "def generate_image_colab_fn(prompt, negative_prompt, style_name, dimensions_str, \n",
    "                            num_inference_steps, guidance_scale, seed_value, \n",
    "                            custom_filename_prefix_val, save_to_gdrive_val, \n",
    "                            # upscaler_active_val, # For later upscaling step\n",
    "                            progress=gr.Progress(track_ τότε=True)):\n",
    "    global pipe, current_model_id, loaded_model_type, GDRIVE_MOUNTED_SUCCESSFULLY, GDRIVE_SAVE_PATH\n",
    "    \n",
    "    additional_info = []\n",
    "    if pipe is None:\n",
    "        return None, \"Model not loaded. Please run the model loading cell first.\", gr.DownloadButton.update(visible=False)\n",
    "\n",
    "    progress(0, desc=\"🎨 Starting generation...\")\n",
    "    width, height = parse_dimensions(dimensions_str)\n",
    "    additional_info.append(f\"Requested dimensions: {width}x{height}\")\n",
    "\n",
    "    # Dimension compatibility check (simple version)\n",
    "    if loaded_model_type == \"sd1.5\" and (width > 768 or height > 768):\n",
    "        additional_info.append(f\"Warning: SD 1.5 works best at 512x512 or 768x768. Requested: {width}x{height}. Adjusting to 512x512 for stability.\")\n",
    "        width, height = 512, 512\n",
    "    elif loaded_model_type == \"sdxl\" and (width < 768 or height < 768):\n",
    "         additional_info.append(f\"Warning: SDXL typically performs better at 1024x1024 or 768x768. Requested: {width}x{height}. Results may vary.\")\n",
    "    # On free tier Colab (T4), 1024x1024 for SDXL might be very slow or cause OOM.\n",
    "    # Consider adding a check for GPU type if more fine-grained control is needed.\n",
    "\n",
    "    styled_prompt, style_negative_prefix = apply_style(prompt, style_name)\n",
    "    final_negative_prompt = negative_prompt\n",
    "    if style_negative_prefix:\n",
    "        final_negative_prompt = f\"{style_negative_prefix}, {negative_prompt}\" if negative_prompt else style_negative_prefix\n",
    "\n",
    "    progress(0.1, desc=f\"📝 Prompt: {styled_prompt[:100]}...\")\n",
    "    print(f\"Generating with Prompt: '{styled_prompt}' at {width}x{height}\")\n",
    "    if final_negative_prompt: print(f\"Negative Prompt: '{final_negative_prompt}'\")\n",
    "    \n",
    "    try:\n",
    "        seed = int(seed_value)\n",
    "    except (ValueError, TypeError):\n",
    "        seed = get_random_seed()\n",
    "    print(f\"🌱 Seed: {seed}\")\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    generator = torch.Generator(device).manual_seed(seed)\n",
    "    num_inference_steps = int(num_inference_steps)\n",
    "    guidance_scale = float(guidance_scale)\n",
    "\n",
    "    generation_args = {\n",
    "        \"prompt\": styled_prompt,\n",
    "        \"negative_prompt\": final_negative_prompt if final_negative_prompt else None,\n",
    "        \"num_inference_steps\": num_inference_steps,\n",
    "        \"guidance_scale\": guidance_scale,\n",
    "        \"generator\": generator,\n",
    "        \"width\": width,\n",
    "        \"height\": height\n",
    "    }\n",
    "\n",
    "    image = None\n",
    "    try:\n",
    "        with torch.autocast(\"cuda\", enabled=torch.cuda.is_available()): # Handles float16 context\n",
    "            for i in progress.tqdm(range(num_inference_steps), desc=\" Diffusion steps\"):\n",
    "                if i == num_inference_steps - 1:\n",
    "                    image = pipe(**generation_args).images[0]\n",
    "        \n",
    "        if image is None: raise RuntimeError(\"Image generation failed within loop.\")\n",
    "\n",
    "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        # Create a prompt prefix for the filename, max 30 chars, alphanumeric only\n",
    "        sane_prompt_words = \"\".join(c if c.isalnum() or c.isspace() else \" \" for c in styled_prompt).split()\n",
    "        prompt_filename_prefix = \"_\".join(sane_prompt_words[:5])[:30] # First 5 words, max 30 chars\n",
    "        if not prompt_filename_prefix: prompt_filename_prefix = \"generated_image\"\n",
    "\n",
    "        filename_prefix_to_use = custom_filename_prefix_val.strip() if custom_filename_prefix_val.strip() else prompt_filename_prefix\n",
    "        base_filename = f\"{filename_prefix_to_use}_{width}x{height}_{seed}_{timestamp}.png\"\n",
    "\n",
    "        print(f\"✅ Image generation successful. Filename: {base_filename}\")\n",
    "        progress(1.0, desc=\"🎉 Generation Complete! Initial image ready.\")\n",
    "        \n",
    "        # Placeholder for upscaling logic - to be added in next step\n",
    "        # if upscaler_active_val:\n",
    "        #     progress(0.0, desc=\"Upscaling image...\") # Reset progress for upscaling\n",
    "        #     image = upscale_image_function(image, progress) # This function will also use progress\n",
    "        #     additional_info.append(\"Image upscaled.\")\n",
    "        #     base_filename = f\"{filename_prefix_to_use}_{image.width}x{image.height}_upscaled_{seed}_{timestamp}.png\"\n",
    "        #     print(f\"Upscaled image. New filename: {base_filename}\")\n",
    "        #     progress(1.0, desc=\"🎉 Upscaling Complete!\")\n",
    "\n",
    "        temp_dir = \"temp_generated_colab_images\"\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        local_img_path = os.path.join(temp_dir, base_filename)\n",
    "        image.save(local_img_path)\n",
    "        additional_info.append(f\"Saved locally for download: {local_img_path}\")\n",
    "\n",
    "        if save_to_gdrive_val and 'google.colab' in sys.modules:\n",
    "            if not GDRIVE_MOUNTED_SUCCESSFULLY: GDRIVE_MOUNTED_SUCCESSFULLY = mount_google_drive()\n",
    "            if GDRIVE_MOUNTED_SUCCESSFULLY:\n",
    "                try:\n",
    "                    os.makedirs(GDRIVE_SAVE_PATH, exist_ok=True)\n",
    "                    gdrive_img_path = os.path.join(GDRIVE_SAVE_PATH, base_filename)\n",
    "                    image.save(gdrive_img_path)\n",
    "                    additional_info.append(f\"Saved to GDrive: {gdrive_img_path}\")\n",
    "                except Exception as e_gdrive:\n",
    "                    additional_info.append(f\"❌ Error saving to Google Drive: {e_gdrive}\")\n",
    "            else:\n",
    "                additional_info.append(\"❌ Google Drive not mounted. Cannot save.\")\n",
    "        elif save_to_gdrive_val:\n",
    "            additional_info.append(\"ℹ️ 'Save to Google Drive' is Colab-only.\")\n",
    "        \n",
    "        img_w, img_h = image.size\n",
    "        info_text = f\"🖼️ Size: {img_w}x{img_h}\\n🌱 Seed: {seed}\\n🕒 Timestamp: {timestamp}\\n🔧 Model: {current_model_id} ({loaded_model_type})\\n🎨 Style: {style_name}\\n📛 Filename: {base_filename}\"\n",
    "        full_info_text = info_text + \"\\n\\n**Logs:**\\n\" + \"\\n\".join(additional_info)\n",
    "        return image, full_info_text, gr.DownloadButton.update(value=local_img_path, visible=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        error_message = f\"❌ Error during generation: {str(e)}\"\n",
    "        print(error_message)\n",
    "        progress(1.0, desc=error_message)\n",
    "        return None, f\"{error_message}. Check Colab console.\", gr.DownloadButton.update(visible=False)\n",
    "\n",
    "print(\"Helper functions and generation logic defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Gradio User Interface\n",
    "\n",
    "This cell sets up the interactive web UI using Gradio. It includes input fields for prompts, style selection, sliders for generation parameters, seed control, and areas for displaying the generated image and status messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title 3. Define Gradio User Interface\n",
    "\n",
    "status_textbox_ref = [None] \n",
    "\n",
    "def create_gradio_ui_colab():\n",
    "    global status_textbox_ref\n",
    "    # Define image dimension choices\n",
    "    dimension_choices = [\"512x512\", \"512x768\", \"768x512\", \"768x768\", \"1024x1024\", \"768x1024\", \"1024x768\"]\n",
    "    default_dimension = \"512x512\"\n",
    "\n",
    "    with gr.Blocks(theme=gr.themes.Soft(), css=\".gradio-container { max-width: 98% !important; } footer {display: none !important}\") as demo:\n",
    "        gr.Markdown(f\"# 🎨 Ultra-Professional Stable Diffusion UI (Colab)\")\n",
    "        status_textbox_ref[0] = gr.Textbox(label=\"Status\", value=\"Interface loaded. Please load a model using Cell 4.\", interactive=False, lines=2)\n",
    "\n",
    "        with gr.Row(equal_height=False):\n",
    "            with gr.Column(scale=2, min_width=450): # Adjusted min_width for more controls\n",
    "                gr.Markdown(\"## ⚙️ Input Controls\")\n",
    "                prompt_input = gr.Textbox(label=\"Enter your Prompt\", lines=3, placeholder=\"e.g., A majestic lion in a futuristic city, neon lights, detailed fur\")\n",
    "                negative_prompt_input = gr.Textbox(label=\"Negative Prompt (what to avoid)\", lines=2, placeholder=\"e.g., blurry, low quality, ugly, text, watermark, disfigured, cartoon\")\n",
    "                style_dropdown = gr.Dropdown(label=\"Artistic Style Preset\", choices=[\"None\"] + list(STYLE_PRESETS.keys()), value=\"None\") # Updated to use new STYLE_PRESETS\n",
    "\n",
    "                dimensions_dropdown = gr.Dropdown(label=\"Image Dimensions (Width x Height)\", choices=dimension_choices, value=default_dimension, \n",
    "                                                info=\"SD1.5 best at 512/768. SDXL best at 1024. Larger sizes need more VRAM & time.\")\n",
    "\n",
    "                with gr.Row():\n",
    "                    inference_steps_slider = gr.Slider(minimum=10, maximum=50, value=25, step=1, label=\"Inference Steps\", info=\"Usually 20-40. More steps = more detail but longer.\")\n",
    "                    cfg_scale_slider = gr.Slider(minimum=1.0, maximum=15.0, value=7.0, step=0.5, label=\"CFG Scale (Guidance)\", info=\"5-10 is typical. How strongly to follow prompt.\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    seed_input = gr.Number(label=\"Seed\", value=get_random_seed(), precision=0, minimum=-1) # Allow -1 for random in some pipelines\n",
    "                    random_seed_button = gr.Button(\"🎲 Randomize Seed\", scale=1, min_width=50)\n",
    "                \n",
    "                with gr.Accordion(\"Output & Advanced Options\", open=True):\n",
    "                    custom_filename_input = gr.Textbox(label=\"Custom Filename Prefix (Optional)\", placeholder=\"my_creation_prefix\")\n",
    "                    save_to_gdrive_checkbox = gr.Checkbox(label=\"Save to Google Drive\", value=False, visible='google.colab' in sys.modules, info=f\"Saves to {GDRIVE_SAVE_PATH} if Drive is mounted.\")\n",
    "                    # upscale_checkbox = gr.Checkbox(label=\"Enable Upscaling (Real-ESRGAN x4)\", value=False, info=\"Increases generation time.\") # For later\n",
    "                    if 'google.colab' not in sys.modules:\n",
    "                        gr.Markdown(\"_(Google Drive option only available in Colab environment.)_\")\n",
    "\n",
    "                generate_button = gr.Button(\"🖼️ Generate Image\", variant=\"primary\")\n",
    "\n",
    "            with gr.Column(scale=3, min_width=520):\n",
    "                gr.Markdown(\"## 🖼️ Generated Image\")\n",
    "                image_output = gr.Image(label=\"Output Image\", type=\"pil\", height=512, show_label=False, show_download_button=False, visible=False)\n",
    "                download_button = gr.DownloadButton(label=\"💾 Download Image\", visible=False)\n",
    "                info_output = gr.Textbox(label=\"Generation Info & Logs\", lines=7, interactive=False) # Increased lines for more info\n",
    "\n",
    "        def on_generate_wrapper(prompt, neg_prompt, style, dimensions, steps, cfg, seed, filename_prefix, save_gdrive, progress=gr.Progress(track_ τότε=True)):\n",
    "            yield {\n",
    "                generate_button: gr.update(interactive=False, value=\"⏳ Generating...\"),\n",
    "                status_textbox_ref[0]: gr.update(value=\"⏳ Generating image...\"),\n",
    "                image_output: gr.update(visible=False, value=None),\n",
    "                download_button: gr.update(visible=False),\n",
    "                info_output: \"\"\n",
    "            }\n",
    "            \n",
    "            img, info_text, dl_button_update = generate_image_colab_fn(prompt, neg_prompt, style, dimensions, steps, cfg, seed, filename_prefix, save_gdrive, progress=progress)\n",
    "            \n",
    "            yield {\n",
    "                image_output: gr.update(value=img, visible=True if img else False),\n",
    "                info_output: info_text,\n",
    "                generate_button: gr.update(interactive=True, value=\"🖼️ Generate Image\"),\n",
    "                status_textbox_ref[0]: gr.update(value=\"✅ Generation complete.\" if img else \"❌ Generation failed. Check console.\"),\n",
    "                download_button: dl_button_update\n",
    "            }\n",
    "\n",
    "        generate_button.click(\n",
    "            fn=on_generate_wrapper,\n",
    "            inputs=[\n",
    "                prompt_input, negative_prompt_input, style_dropdown, dimensions_dropdown, \n",
    "                inference_steps_slider, cfg_scale_slider, seed_input, \n",
    "                custom_filename_input, save_to_gdrive_checkbox\n",
    "            ],\n",
    "            outputs=[generate_button, status_textbox_ref[0], image_output, download_button, info_output]\n",
    "        )\n",
    "\n",
    "        random_seed_button.click(fn=get_random_seed, inputs=None, outputs=seed_input)\n",
    "        \n",
    "        gr.Markdown(\"--- \")\n",
    "        gr.Markdown(\"**Note:** If you encounter issues, check the Colab console output for error messages. \"\n",
    "                    \"Ensure you have selected a GPU runtime (`Runtime` -> `Change runtime type` -> `GPU`).\")\n",
    "        gr.Markdown(\"--- \")\n",
    "        gr.Markdown(\"### Running Locally with `app.py`:\\n\"\n",
    "                    \"1. Download `app.py` and `requirements.txt` (if provided, or install: `pip install gradio torch diffusers transformers accelerate Pillow`).\\n\"\n",
    "                    \"2. Ensure you have Python and pip installed.\\n\"\n",
    "                    \"3. If using a GPU, ensure CUDA drivers and PyTorch with CUDA support are installed.\\n\"\n",
    "                    \"4. Open your terminal or command prompt, navigate to the directory where you saved `app.py`.\\n\"\n",
    "                    \"5. Run the app: `gradio app.py` or `python app.py`.\\n\"\n",
    "                    \"   You can also pass arguments, e.g., `python app.py --model_id stabilityai/stable-diffusion-2-1-base --port 7861`.\")\n",
    "\n",
    "    return demo\n",
    "\n",
    "gradio_app_instance = create_gradio_ui_colab()\n",
    "print(\"Gradio UI defined. Status textbox referenced.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load the Stable Diffusion Model\n",
    "\n",
    "This cell will download and load the pre-trained Stable Diffusion model. This can take several minutes, especially the first time you run it, as it needs to download the model weights (several gigabytes).\n",
    "\n",
    "**Choose your model below.** `runwayml/stable-diffusion-v1-5` is recommended for the free Colab tier (T4 GPU). SDXL models like `stabilityai/sdxl-base-1.0` are higher quality but require more VRAM (may not fit on T4, might need Colab Pro with A100/V100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title 4. Load the Model (can take a few minutes)\n",
    "\n",
    "# --- Model Configuration ---\n",
    "MODEL_TO_LOAD = \"runwayml/stable-diffusion-v1-5\"  # @param [\"runwayml/stable-diffusion-v1-5\", \"stabilityai/sdxl-base-1.0\", \"dreamlike-art/dreamlike-photoreal-2.0\", \"prompthero/openjourney\"] {\"allow-input\": true}\n",
    "USE_FLOAT16_PRECISION = True  # @param {type:\"boolean\"}\n",
    "ENABLE_ATTENTION_SLICING = True  # @param {type:\"boolean\"}\n",
    "\n",
    "print(f\"Selected model for loading: {MODEL_TO_LOAD}\")\n",
    "print(f\"Using float16 precision: {USE_FLOAT16_PRECISION}\")\n",
    "print(f\"Enabling attention slicing: {ENABLE_ATTENTION_SLICING}\")\n",
    "\n",
    "# Clean up any existing pipe to free memory before loading a new one\n",
    "if 'pipe' in globals() and pipe is not None:\n",
    "    print(\"Clearing existing model from memory...\")\n",
    "    del pipe\n",
    "    pipe = None # Ensure it's None\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Existing model cleared.\")\n",
    "\n",
    "# Update the global current_model_id for the info display\n",
    "current_model_id = MODEL_TO_LOAD\n",
    "\n",
    "# The load_model_colab is a generator. We iterate through its yields to process them.\n",
    "# The status updates are handled by yielding gr.update() for the status_textbox.\n",
    "if status_textbox_ref[0] is not None:\n",
    "    # This is tricky. Direct update of status_textbox_ref[0] from here won't reflect in UI\n",
    "    # unless this cell's execution is part of a Gradio event chain that has status_textbox_ref[0] as an output.\n",
    "    # For a standalone cell execution, print() is the most reliable feedback during the process.\n",
    "    # The yielded gr.update() values are for when this function is called *by* Gradio.\n",
    "    # We will manually set the value of the referenced textbox at the end of this cell.\n",
    "    status_textbox_ref[0].value = f\"Initiating load for {MODEL_TO_LOAD}...\" # Initial message\n",
    "    print(f\"UI Status Check: Textbox for status is available. Initial message set.\")\n",
    "\n",
    "    final_status_message = f\"Model {MODEL_TO_LOAD} loading process initiated.\"\n",
    "    # Consume the generator to execute the loading process\n",
    "    for status_update_yield in load_model_colab(MODEL_TO_LOAD, USE_FLOAT16_PRECISION, ENABLE_ATTENTION_SLICING, status_textbox_ref):\n",
    "        # The yielded gr.update object contains the value for the status textbox\n",
    "        if isinstance(status_update_yield, gr. uomini.Update):\n",
    "            final_status_message = status_update_yield.value\n",
    "            # We can't apply this update to the UI directly from here in a way that Gradio recognizes for a live update.\n",
    "            # Print it to console for confirmation.\n",
    "            print(f\"Loading progress: {final_status_message}\")\n",
    "        else:\n",
    "            # This case should not happen if load_model_colab always yields gr.update\n",
    "            print(f\"Unexpected yield from load_model_colab: {status_update_yield}\")\n",
    "    \n",
    "    # After the loop, set the final status on the referenced textbox\n",
    "    if status_textbox_ref[0] is not None:\n",
    "        status_textbox_ref[0].value = final_status_message # Manually update the value property\n",
    "        print(f\"Final status after loading attempt: {final_status_message}\")\n",
    "    \n",
    "    if pipe is None:\n",
    "        print(f\"Critical: Model {MODEL_TO_LOAD} FAILED to load. Check console output above for errors.\")\n",
    "        if status_textbox_ref[0] is not None: status_textbox_ref[0].value = f\"❌ Model {MODEL_TO_LOAD} FAILED to load. Check console.\"\n",
    "    else:\n",
    "        print(f\"Success: Model {MODEL_TO_LOAD} loaded and ready.\")\n",
    "        if status_textbox_ref[0] is not None: status_textbox_ref[0].value = f\"✅ Model '{MODEL_TO_LOAD}' loaded. Ready to generate.\"\n",
    "else:\n",
    "    print(\"Warning: Gradio status textbox reference not found. Loading model without live UI status updates during load.\")\n",
    "    # Fallback if status_textbox_ref[0] isn't correctly captured (should not happen with current setup)\n",
    "    for _ in load_model_colab(MODEL_TO_LOAD, USE_FLOAT16_PRECISION, ENABLE_ATTENTION_SLICING, None): pass # Consume generator\n",
    "    if pipe is None: print(f\"Critical: Model {MODEL_TO_LOAD} FAILED to load (no UI ref). Check console.\")\n",
    "    else: print(f\"Success: Model {MODEL_TO_LOAD} loaded (no UI ref). Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Launch the Gradio App\n",
    "\n",
    "Run the cell below to start the Gradio interface. \n",
    "\n",
    "**Important:** \n",
    "*   Make sure **Cell 4 (Load the Model)** has completed successfully before running this cell.\n",
    "*   Click the **public URL** (it usually looks like `https://xxxx.gradio.live` or `https://xxxxxx.gradio.app`) that appears in the output to open the web UI in a new tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @title 5. Launch Gradio UI\n",
    "if pipe is not None and gradio_app_instance is not None:\n",
    "    final_launch_message = f\"🚀 Launching Gradio app with model: {current_model_id}\"\n",
    "    print(final_launch_message)\n",
    "    if status_textbox_ref[0] is not None:\n",
    "        status_textbox_ref[0].value = final_launch_message # Update status before launching\n",
    "    \n",
    "    # Clean up temp directory from previous runs if any\n",
    "    temp_image_dir = \"temp_generated_colab_images\"\n",
    "    if os.path.exists(temp_image_dir):\n",
    "        try:\n",
    "            import shutil\n",
    "            shutil.rmtree(temp_image_dir)\n",
    "            print(f\"Cleaned up old temp directory: {temp_image_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not clean up temp directory {temp_image_dir}: {e}\")\n",
    "    os.makedirs(temp_image_dir, exist_ok=True) # Ensure it exists for current session\n",
    "\n",
    "    gradio_app_instance.queue().launch(debug=False, share=True) # share=True creates a public link\n",
    "else:\n",
    "    error_msg = \"ERROR: Model not loaded or Gradio UI not defined. Cannot launch Gradio app.\"\n",
    "    print(error_msg)\n",
    "    if status_textbox_ref[0] is not None:\n",
    "        status_textbox_ref[0].value = error_msg\n",
    "    print(\"Please ensure Cell 3 (Define UI) and Cell 4 (Load Model) executed successfully.\")\n",
    "    if pipe is None: print(\"- Model (pipe) is None.\")\n",
    "    if gradio_app_instance is None: print(\"- Gradio App (gradio_app_instance) is None.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
