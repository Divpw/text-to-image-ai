{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion Image Generation with Gradio UI\n",
    "\n",
    "This notebook allows you to generate images from text prompts using Stable Diffusion. It runs on a free Google Colab GPU and provides a Gradio interface for interaction.\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Enable GPU:** Go to `Runtime` -> `Change runtime type` and select `GPU` (e.g., T4) as the hardware accelerator.\n",
    "2.  **Run Cells:** Execute each cell in order.\n",
    "    *   The first cell installs necessary libraries.\n",
    "    *   The second cell imports libraries and defines the image generation function and Gradio UI.\n",
    "    *   The third cell loads the Stable Diffusion model (this may take a few minutes, especially on the first run as it downloads model weights).\n",
    "    *   The fourth cell launches the Gradio interface. Click the public URL it provides to open the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1. Install Dependencies\n",
    "!pip install diffusers transformers accelerate gradio bitsandbytes --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Define Generation Function & UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 2. Import Libraries, Define Generation Function & UI\n",
    "import gradio as gr\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Global variable for the pipeline\n",
    "pipe = None\n",
    "model_id = \"stabilityai/stable-diffusion-2-1-base\" # You can change this to other models like \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Loads the Stable Diffusion model.\"\"\"\n",
    "    global pipe\n",
    "    if pipe is None:\n",
    "        print(f\"Loading model: {model_id}...\")\n",
    "        scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "        try:\n",
    "            pipe = StableDiffusionPipeline.from_pretrained(\n",
    "                model_id,\n",
    "                scheduler=scheduler,\n",
    "                torch_dtype=torch.float16,\n",
    "                # load_in_8bit=True, # Enable this for lower VRAM usage, requires bitsandbytes\n",
    "                # device_map=\"auto\"\n",
    "            )\n",
    "            pipe = pipe.to(\"cuda\")\n",
    "            # pipe.enable_attention_slicing() # Enable for further VRAM reduction at a slight performance cost\n",
    "            print(\"Model loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            pipe = None # Ensure pipe is None if loading failed\n",
    "    else:\n",
    "        print(\"Model already loaded.\")\n",
    "\n",
    "def generate_image(prompt, negative_prompt, seed_value, num_inference_steps=25, guidance_scale=7.5):\n",
    "    \"\"\"Generates an image based on the prompt, negative prompt, and seed.\"\"\"\n",
    "    if pipe is None:\n",
    "        return None, \"Model not loaded. Please run the model loading cell first.\"\n",
    "    \n",
    "    print(f\"Generating image with prompt: '{prompt}'\")\n",
    "    if negative_prompt:\n",
    "        print(f\"Negative prompt: '{negative_prompt}'\")\n",
    "    \n",
    "    # Use provided seed or generate a random one if empty or invalid\n",
    "    try:\n",
    "        seed = int(seed_value)\n",
    "    except (ValueError, TypeError):\n",
    "        seed = random.randint(0, 2**32 - 1)\n",
    "    print(f\"Using seed: {seed}\")\n",
    "    \n",
    "    generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
    "    \n",
    "    try:\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            image = pipe(\n",
    "                prompt,\n",
    "                negative_prompt=negative_prompt if negative_prompt else None,\n",
    "                num_inference_steps=int(num_inference_steps),\n",
    "                guidance_scale=guidance_scale,\n",
    "                generator=generator\n",
    "            ).images[0]\n",
    "        print(\"Image generated.\")\n",
    "        return image, f\"Seed used: {seed}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error during image generation: {e}\")\n",
    "        return None, f\"Error: {e}\"\n",
    "\n",
    "# Gradio Interface Definition\n",
    "with gr.Blocks(css=\"footer {display: none !important}\") as demo:\n",
    "    gr.Markdown(\"# Stable Diffusion Image Generator\")\n",
    "    gr.Markdown(\"Enter a prompt and a negative prompt, then click 'Generate Image'.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            prompt_input = gr.Textbox(label=\"Prompt\", placeholder=\"e.g., A photo of an astronaut riding a horse on the moon\")\n",
    "            negative_prompt_input = gr.Textbox(label=\"Negative Prompt (Optional)\", placeholder=\"e.g., blurry, low quality, ugly, text, watermark\")\n",
    "            seed_input = gr.Textbox(label=\"Seed (Optional)\", placeholder=\"Enter a number or leave blank for random\")\n",
    "            with gr.Row():\n",
    "                 num_steps_slider = gr.Slider(minimum=10, maximum=100, value=25, step=1, label=\"Number of Inference Steps\")\n",
    "                 guidance_slider = gr.Slider(minimum=1, maximum=20, value=7.5, step=0.1, label=\"Guidance Scale\")\n",
    "            generate_button = gr.Button(\"Generate Image\", variant=\"primary\")\n",
    "        with gr.Column(scale=2):\n",
    "            image_output = gr.Image(label=\"Generated Image\", type=\"pil\", show_download_button=True)\n",
    "            status_output = gr.Textbox(label=\"Status\", interactive=False)\n",
    "\n",
    "    generate_button.click(\n",
    "        generate_image, \n",
    "        inputs=[prompt_input, negative_prompt_input, seed_input, num_steps_slider, guidance_slider],\n",
    "        outputs=[image_output, status_output]\n",
    "    )\n",
    "    \n",
    "    gr.Markdown(\"## Tips for Better Prompts:\n",
    "    - Be specific: 'A hyperrealistic 4K photo of a red apple on a wooden table' is better than 'apple'.\n",
    "    - Use artistic styles: '...in the style of Van Gogh', '...as a watercolor painting', '...cyberpunk art'.\n",
    "    - Add details: '...with dramatic lighting', '...detailed fur', '...wearing a tiny hat'.\n",
    "    - Use negative prompts to exclude unwanted elements: 'ugly, blurry, watermark, text, disfigured'.\")\n",
    "\n",
    "print(\"Gradio UI defined. Next, load the model and then launch the app.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the Stable Diffusion Model\n",
    "\n",
    "This cell will download and load the pre-trained Stable Diffusion model. This can take several minutes, especially the first time you run it, as it needs to download the model weights (several gigabytes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 3. Load the Model (can take a few minutes)\n",
    "load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Launch the Gradio App\n",
    "\n",
    "Run the cell below to start the Gradio interface. Click the public URL (it usually looks like `https://xxxx.gradio.live`) that appears in the output to open the web UI in a new tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 4. Launch Gradio UI\n",
    "if pipe is not None:\n",
    "    demo.launch(debug=True, share=True) # share=True creates a public link\n",
    "else:\n",
    "    print(\"ERROR: Model not loaded. Cannot launch Gradio app. Please ensure the previous cell executed successfully and the model was loaded.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
